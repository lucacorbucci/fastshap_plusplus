{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUTCH Fastshap vs FastShap with DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  # https://github.com/slundberg/shap\n",
    "import shapreg  # https://github.com/iancovert/shapley-regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import torch\n",
    "import dill\n",
    "# import from parent level\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from fastshap import FastSHAP\n",
    "from scipy.stats import sem\n",
    "from fastshap.utils import MaskLayer1d\n",
    "import torch.nn as nn\n",
    "from fastshap import Surrogate, KLDivLoss\n",
    "from scipy.stats import spearmanr\n",
    "import shap  # https://github.com/slundberg/shap\n",
    "import shapreg  # https://github.com/iancovert/shapley-regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import torch\n",
    "from fastshap import FastSHAP\n",
    "from scipy.stats import sem\n",
    "from fastshap.utils import MaskLayer1d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastshap import Surrogate, KLDivLoss\n",
    "from aix360.metrics.local_metrics import faithfulness_metric\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from Utils.utils import Utils\n",
    "from Client.client_explainer import prepare_dataset_for_explainer_FL\n",
    "\n",
    "from metrics import agreement_fraction, pairwise_rank_agreement, rankcorr\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastshap.surrogate_dp import SurrogateDP\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from pathlib import Path\n",
    "from fastshap.fastshap_dp import (\n",
    "    FastSHAP,\n",
    "    calculate_grand_coalition_FL,\n",
    "    validate_FL,\n",
    ")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "import warnings\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Legend Plots\n",
    "legendFig = plt.figure(\"Legend plot\")\n",
    "# Example plot for legend creation\n",
    "colors = plt.cm.Set2(range(20))  # Using Set2 palette\n",
    "markers = [\"o\", \"s\", \"^\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "green_marker = Line2D([], [], color=colors[0], marker=markers[-2], markersize=10, linestyle=\"None\")\n",
    "orange_marker = Line2D([], [], color=colors[1], marker=markers[0], markersize=10, linestyle=\"None\")\n",
    "\n",
    "line = Line2D([], [], color=\"purple\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "legend_fig = plt.figure(figsize=(18, 4))\n",
    "legendFig.legend(\n",
    "    (\n",
    "        [line, orange_marker, green_marker]\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            \"W/o Privacy VS Semi Private\",\n",
    "            \"W/o Privacy VS Full Private\",\n",
    "            \"Semi Private VS Full Private\",\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    ncol=3,\n",
    "    # handler_map={\n",
    "    #     baseline_line: HandlerDashedLineWithMarker(),\n",
    "    #     baseline_DP_line: HandlerDashedLineWithMarker(),\n",
    "    #     tunable_line: HandlerDashedLineWithMarker(),\n",
    "    # },\n",
    "    # handlelength=5,\n",
    "    prop={\"family\": \"monospace\", \"size\": 25},\n",
    ")\n",
    "# legendFig.show()\n",
    "legendFig.savefig(f\"./legend_plots.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Legend Plots\n",
    "legendFig = plt.figure(\"Legend plot\")\n",
    "# Example plot for legend creation\n",
    "colors = plt.cm.Set2(range(20))  # Using Set2 palette\n",
    "markers = [\"o\", \"s\", \"^\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "green_marker = Line2D([], [], color=colors[0], marker=markers[-2], markersize=10, linestyle=\"None\")\n",
    "orange_marker = Line2D([], [], color=colors[1], marker=markers[0], markersize=10, linestyle=\"None\")\n",
    "\n",
    "line = Line2D([], [], color=\"purple\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "legend_fig = plt.figure(figsize=(18, 4))\n",
    "legendFig.legend(\n",
    "    (\n",
    "        [line, orange_marker, green_marker]\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            \"W/o Privacy VS Semi Private\",\n",
    "            \"W/o Privacy VS Full Private\",\n",
    "            \"Semi Private VS Full Private\",\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    ncol=3,\n",
    "    # handler_map={\n",
    "    #     baseline_line: HandlerDashedLineWithMarker(),\n",
    "    #     baseline_DP_line: HandlerDashedLineWithMarker(),\n",
    "    #     tunable_line: HandlerDashedLineWithMarker(),\n",
    "    # },\n",
    "    # handlelength=5,\n",
    "    prop={\"size\": 25},\n",
    ")\n",
    "# legendFig.show()\n",
    "legendFig.savefig(f\"./legend_plots_no_monospace.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_FL(\n",
    "    test_clients,\n",
    "    batch_size,\n",
    "    dataset_name,\n",
    "    num_workers,\n",
    "    fed_dir,\n",
    "    device,\n",
    "    num_features,\n",
    "    surrogate,\n",
    "    seed=42,\n",
    "):\n",
    "    samples, targets = [], []\n",
    "    for client in test_clients:\n",
    "        data = Utils.get_dataset(\n",
    "            path_to_data=Path(fed_dir),\n",
    "            cid=client,\n",
    "            dataset=dataset_name,\n",
    "            partition=\"train\",\n",
    "        )\n",
    "        for item in data:\n",
    "            X, _, y = item\n",
    "            samples.append(X)\n",
    "            targets.append(y)\n",
    "\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faithfulness evaluation\n",
    "class aix_model:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x).argmax(dim=1)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # since the activation function of the last layer is LogSoftmax\n",
    "        # we need to apply the exponential to the output of the model\n",
    "        # cast x to be a Tensor\n",
    "        x = torch.Tensor(x)\n",
    "        return torch.functional.softmax(self.model(x)).detach().numpy()\n",
    "\n",
    "\n",
    "def compute_faithfulness(x, y, fastshap_explanation, model, base_value):\n",
    "    x = x[0]\n",
    "    fastshap_explanation = np.array(torch.tensor(fastshap_explanation).cpu())\n",
    "\n",
    "    faithfulness = faithfulness_metric(\n",
    "        model=model,\n",
    "        x=np.array(x),\n",
    "        coefs=fastshap_explanation,\n",
    "        base=base_value * np.ones(shape=fastshap_explanation.shape[0]),\n",
    "    )\n",
    "    return faithfulness\n",
    "\n",
    "\n",
    "def process_explainer(\n",
    "    explanation_file_name, explainer, model, x, y, index, base_value, base_path\n",
    "):\n",
    "    if not os.path.isfile(\n",
    "        f\"{base_path}/explanations/{explanation_file_name}_{index}.pt\"\n",
    "    ):\n",
    "        fastshap_explanation = explainer.shap_values(x)[0][:, y]\n",
    "        torch.save(\n",
    "            fastshap_explanation,\n",
    "            f\"{base_path}/explanations/{explanation_file_name}_{index}.pt\",\n",
    "        )\n",
    "    else:\n",
    "        fastshap_explanation = torch.load(\n",
    "            f\"{base_path}/explanations/{explanation_file_name}_{index}.pt\"\n",
    "        )\n",
    "\n",
    "    if not os.path.isfile(\n",
    "        f\"{base_path}/faithfulness/{explanation_file_name}_{index}_{base_value}.pt\"\n",
    "    ):\n",
    "        faithfulness = compute_faithfulness(\n",
    "            x,\n",
    "            y,\n",
    "            fastshap_explanation,\n",
    "            model,\n",
    "            base_value=base_value,\n",
    "        )\n",
    "        torch.save(\n",
    "            faithfulness,\n",
    "            f\"{base_path}/faithfulness/{explanation_file_name}_{index}_{base_value}.pt\",\n",
    "        )\n",
    "    else:\n",
    "        faithfulness = torch.load(\n",
    "            f\"{base_path}/faithfulness/{explanation_file_name}_{index}_{base_value}.pt\"\n",
    "        )\n",
    "    return fastshap_explanation, faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset_name = \"adult\"\n",
    "        self.sweep = True\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_error_bar(\n",
    "    labels,\n",
    "    results_mean,\n",
    "    y_label,\n",
    "    title,\n",
    "    y_lim=None,\n",
    "    x_ticks=None,\n",
    "    results_std=None,\n",
    "    file_name=None,\n",
    "    results_std_surrogate_NO_DP=None,\n",
    "    results_mean_surrogate_NO_DP=None,\n",
    "    jittering=False,\n",
    "    horizontal_line=None,\n",
    "    horizontal_line_std=None\n",
    "):\n",
    "    errors = None\n",
    "    means = [results_mean[label] for label in labels]\n",
    "    if results_std:\n",
    "        errors = [results_std[label] for label in labels]\n",
    "\n",
    "    if results_mean_surrogate_NO_DP:\n",
    "        means_surrogate_NO_DP = [results_mean_surrogate_NO_DP[label] for label in labels]\n",
    "    if results_std_surrogate_NO_DP:\n",
    "        errors_surrogate_NO_DP = [results_std_surrogate_NO_DP[label] for label in labels]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "\n",
    "    colors = plt.cm.Set2(range(len(labels)))  # Using Set2 palette\n",
    "    markers = [\"o\", \"s\", \"^\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "    if errors:\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.errorbar(\n",
    "                i,\n",
    "                means[i],\n",
    "                yerr=errors[i],\n",
    "                fmt=markers[i] if not results_mean_surrogate_NO_DP else markers[-2],\n",
    "                label=label,\n",
    "                capsize=5,\n",
    "                markersize=20,\n",
    "                color=colors[i] if not results_mean_surrogate_NO_DP else colors[0],\n",
    "                linewidth=3,\n",
    "            )\n",
    "    else:\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.errorbar(\n",
    "                i,\n",
    "                means[i],\n",
    "                fmt=markers[i] if not results_mean_surrogate_NO_DP else markers[-2],\n",
    "                label=label,\n",
    "                capsize=5,\n",
    "                markersize=20,\n",
    "                color=colors[i] if not results_mean_surrogate_NO_DP else colors[0],\n",
    "                linewidth=3,\n",
    "            )\n",
    "    jitter = 0.13\n",
    "    if results_mean_surrogate_NO_DP:\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.errorbar(\n",
    "                i+jitter,\n",
    "                means_surrogate_NO_DP[i],\n",
    "                yerr=errors_surrogate_NO_DP[i],\n",
    "                fmt=markers[0],\n",
    "                label=label,\n",
    "                capsize=5,\n",
    "                markersize=20,\n",
    "                color=colors[1],\n",
    "                linewidth=3,\n",
    "            )\n",
    "    else:\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.errorbar(\n",
    "                i,\n",
    "                means_surrogate_NO_DP[i],\n",
    "                fmt=markers[0],\n",
    "                label=label,\n",
    "                capsize=5,\n",
    "                markersize=20,\n",
    "                color=colors[1],\n",
    "                linewidth=3,\n",
    "            )\n",
    "\n",
    "    # Customizing the plot\n",
    "    plt.ylabel(y_label, fontsize=26)\n",
    "    plt.title(title, fontsize=26)\n",
    "    \n",
    "    if y_lim:\n",
    "        plt.ylim(y_lim)\n",
    "    plt.grid(visible=True, which=\"major\", linestyle=\"--\", linewidth=0.6, alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    if x_ticks:\n",
    "        plt.xticks(range(len(labels)), x_ticks, fontsize=22)\n",
    "    else:\n",
    "        plt.xticks(range(len(labels)), labels, fontsize=22)\n",
    "    \n",
    "    if horizontal_line:\n",
    "        plt.axhline(y=horizontal_line, color=\"purple\", linestyle=\"--\", linewidth=2)\n",
    "        if horizontal_line_std:\n",
    "            plt.axhline(\n",
    "                y=horizontal_line + horizontal_line_std,\n",
    "                color=\"purple\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "            plt.axhline(\n",
    "                y=horizontal_line - horizontal_line_std,\n",
    "                color=\"purple\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "            x_min, x_max = plt.xlim()\n",
    "            x_min -= 0.2\n",
    "            x_max += 0.2\n",
    "            plt.xlim(x_min, x_max)\n",
    "\n",
    "            x_full = np.linspace(x_min, x_max, 500)\n",
    "            plt.fill_between(\n",
    "                x_full,\n",
    "                horizontal_line - horizontal_line_std,\n",
    "                horizontal_line + horizontal_line_std,\n",
    "                color=\"purple\",\n",
    "                alpha=0.05,\n",
    "                interpolate=True\n",
    "          )\n",
    "\n",
    "    plt.yticks(fontsize=22)\n",
    "    plt.gca().yaxis.set_major_locator(plt.MaxNLocator(5))  # Limit the number of yticks\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "    plt.savefig(file_name, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    private_model: bool,\n",
    "    private_surrogate: bool,\n",
    "    explainer_privacy_levels: list,\n",
    "    base_path: str,\n",
    "    model_name: str,\n",
    "    surrogate_name: str,\n",
    "    explainer_name: str,\n",
    "    device: str,\n",
    "    num_features: int,\n",
    "    eps_bb: str = \"\",\n",
    "    no_dp_explainer=None,\n",
    "):\n",
    "    loaded_data = {}\n",
    "    loaded_data[\"private_model\"] = private_model\n",
    "    loaded_data[\"private_surrogate\"] = private_surrogate\n",
    "    private_model_str = \"private_model_\" if private_model else \"\"\n",
    "\n",
    "    if os.path.isfile(\n",
    "        f\"{base_path}bb/{model_name}\"\n",
    "        + (\"_NO_DP\" if not private_model else f\"_{eps_bb}\")\n",
    "        + \".pth\"\n",
    "    ):\n",
    "        print(\"Loading saved model\")\n",
    "        model = torch.load(\n",
    "            f\"{base_path}bb/{model_name}\"\n",
    "            + (\"_NO_DP\" if not private_model else f\"_{eps_bb}\")\n",
    "            + \".pth\"\n",
    "        ).to(\"cpu\")\n",
    "        model_aix = aix_model(model)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Model not found: {base_path}bb/{model_name}\"\n",
    "            + (\"_NO_DP\" if not private_model else f\"_{eps_bb}\")\n",
    "            + \".pth\"\n",
    "        )\n",
    "\n",
    "    if os.path.isfile(\n",
    "        f\"{base_path}/surrogate/{surrogate_name}\"\n",
    "        + \".pth\"\n",
    "    ):\n",
    "        print(f\"Loading saved surrogate model {surrogate_name}.pth\")\n",
    "        surr = torch.load(\n",
    "            f\"{base_path}/surrogate/{surrogate_name}\"\n",
    "            + \".pth\"\n",
    "        ).to(device)\n",
    "        surrogate = Surrogate(surr, num_features)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Surrogate model not found: {base_path}/surrogate/{surrogate_name}\"\n",
    "            + \".pth\"\n",
    "        )\n",
    "    # if no_dp_explainer:\n",
    "    #     print(\"OK\")\n",
    "    #     if os.path.isfile(\n",
    "    #         f\"{base_path}/surrogate/{surrogate_name}_NO_DP.pth\"\n",
    "    #     ):\n",
    "    #         print(\"Loading saved surrogate model without privacy\")\n",
    "    #         surr = torch.load(\n",
    "    #             f\"{base_path}/surrogate/{surrogate_name}_NO_DP.pth\"\n",
    "    #         ).to(device)\n",
    "    #         surrogate = Surrogate(surr, num_features)\n",
    "    #     else:\n",
    "    #         raise ValueError(\"No DP explainer not found\")\n",
    "\n",
    "    loaded_data[\"model\"] = model\n",
    "    loaded_data[\"model_aix\"] = model_aix\n",
    "    loaded_data[\"surrogate\"] = surrogate\n",
    "\n",
    "    if no_dp_explainer:\n",
    "        if os.path.isfile(f\"{base_path}/explainer/{no_dp_explainer}.pth\"):\n",
    "            print(f\"Loading saved explainer model without DP {no_dp_explainer}\")\n",
    "            explainer = torch.load(\n",
    "                f\"{base_path}/explainer/{no_dp_explainer}.pth\"\n",
    "            ).to(device)\n",
    "            fastshap = FastSHAP(\n",
    "                explainer,\n",
    "                surrogate,\n",
    "                normalization=\"none\",\n",
    "                link=nn.Softmax(dim=-1),\n",
    "                num_features=num_features,\n",
    "            )\n",
    "            loaded_data[f\"explainer_NO_DP\"] = fastshap\n",
    "        else:\n",
    "            print(\n",
    "                f\"Explainer model not found: {base_path}/explainer/{no_dp_explainer}.pth\"\n",
    "            )\n",
    "        \n",
    "\n",
    "    for eps in explainer_privacy_levels:\n",
    "        # DP 0.5\n",
    "        if os.path.isfile(f\"{base_path}/explainer/{explainer_name}_{eps}.pth\"):\n",
    "            print(\"Loading saved explainer model\")\n",
    "            explainer = torch.load(\n",
    "                f\"{base_path}/explainer/{explainer_name}_{eps}.pth\"\n",
    "            ).to(device)\n",
    "            fastshap = FastSHAP(\n",
    "                explainer,\n",
    "                surrogate,\n",
    "                normalization=\"none\",\n",
    "                link=nn.Softmax(dim=-1),\n",
    "                num_features=num_features,\n",
    "            )\n",
    "            loaded_data[f\"explainer_{eps}\"] = fastshap\n",
    "        else:\n",
    "            print(\n",
    "                f\"Explainer model not found: {base_path}/explainer/{explainer_name}_{private_model_str}{eps}.pth\"\n",
    "            )\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanations(\n",
    "    model_predictions: list,\n",
    "    X_test,\n",
    "    loaded_data: dict,\n",
    "    base_value: float,\n",
    "    base_path: str,\n",
    "    explainer_privacy_levels: list,\n",
    "):\n",
    "    for privacy_level in explainer_privacy_levels:\n",
    "        faithfulness = []\n",
    "        explanations = []\n",
    "        for index, (x, y) in enumerate(zip(X_test, model_predictions)):\n",
    "            y = 0 if not y else 1\n",
    "            x = np.array([list(x)])\n",
    "\n",
    "            shap_NO_DP, faithfulness_NO_DP = process_explainer(\n",
    "                f\"private_model_{loaded_data['private_model']}_surrogate_{loaded_data['private_surrogate']}_explainer_{privacy_level}\",\n",
    "                loaded_data[f\"explainer_{privacy_level}\"],\n",
    "                loaded_data[\"model_aix\"],\n",
    "                x,\n",
    "                y,\n",
    "                index,\n",
    "                base_value,\n",
    "                base_path=base_path,\n",
    "            )\n",
    "            explanations.append(copy.deepcopy(shap_NO_DP))\n",
    "            faithfulness.append(faithfulness_NO_DP)\n",
    "            loaded_data[f\"explanations_{privacy_level}\"] = explanations\n",
    "            loaded_data[f\"faithfulness_{privacy_level}\"] = faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_base_explainer_shap_values(\n",
    "    loaded_data, explainer_privacy_levels, X_test, feature_names, title, explainer_name\n",
    "):\n",
    "    explanations = []\n",
    "    for index in range(len(X_test)):\n",
    "        fastshap_NO_DP = loaded_data[explainer_name][index]\n",
    "        # compute the absolute values of the shap values\n",
    "        fastshap_NO_DP = np.abs(fastshap_NO_DP)\n",
    "        explanations.append(fastshap_NO_DP)\n",
    "\n",
    "    # mean of the absolute values of the shap values\n",
    "    mean_explanations = np.mean(explanations, axis=0)\n",
    "    # sort the mean_explanations and the corresponding feature names\n",
    "    sorted_indices = np.argsort(mean_explanations)[::-1]\n",
    "    print(len(sorted_indices))\n",
    "    print(len(feature_names))\n",
    "    # plot the mean shap values with the corresponding feature names in sorted indices\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(\n",
    "        [feature_names[i] for i in sorted_indices], mean_explanations[sorted_indices]\n",
    "    )\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Mean SHAP Value\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries_metrics(\n",
    "    explainer_privacy_levels, metrics, current_metric, compute_std=True\n",
    "):\n",
    "    mean = {}\n",
    "    std = {}\n",
    "    std_error = {}\n",
    "    if compute_std:\n",
    "        for privacy_level in explainer_privacy_levels:\n",
    "            mean[\"Epsilon \" + privacy_level.split(\"_\")[1]] = np.mean(\n",
    "                metrics[current_metric + privacy_level]\n",
    "            )\n",
    "            std[\"Epsilon \" + privacy_level.split(\"_\")[1]] = np.std(\n",
    "                metrics[current_metric + privacy_level]\n",
    "            )\n",
    "            std_error[\"Epsilon \" + privacy_level.split(\"_\")[1]] = sem(\n",
    "                metrics[current_metric + privacy_level]\n",
    "            )\n",
    "        return mean, std, std_error\n",
    "    else:\n",
    "        for privacy_level in explainer_privacy_levels:\n",
    "            mean[\"Epsilon \" + privacy_level.split(\"_\")[1]] = metrics[\n",
    "                current_metric + privacy_level\n",
    "            ]\n",
    "\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, X_test):\n",
    "    model_predictions = []\n",
    "    for x in X_test:\n",
    "        model_prediction = model(torch.Tensor(x).unsqueeze(0)).argmax()\n",
    "        model_predictions.append(model_prediction)\n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    X_test,\n",
    "    loaded_data,\n",
    "    explainer_privacy_levels,\n",
    "    selected_features,\n",
    "    all_features,\n",
    "    top_k=None,\n",
    "):\n",
    "    metrics = {}\n",
    "    for privacy_level in explainer_privacy_levels:\n",
    "        L2_distances = []\n",
    "        spearman_correlation = []\n",
    "        cosine_similarity = []\n",
    "        kendall_tau = []\n",
    "        for index in range(len(X_test)):\n",
    "            selected_indexes = [\n",
    "                all_features.index(feature) for feature in selected_features\n",
    "            ]\n",
    "\n",
    "            explanation_NO_DP = loaded_data[\"explanations_NO_DP\"][index][\n",
    "                selected_indexes\n",
    "            ]\n",
    "\n",
    "            explanation_DP = loaded_data[f\"explanations_{privacy_level}\"][index][\n",
    "                selected_indexes\n",
    "            ]\n",
    "\n",
    "            L2_distances.append(np.linalg.norm(explanation_NO_DP - explanation_DP))\n",
    "            coef, p = spearmanr(explanation_DP, explanation_NO_DP)\n",
    "            spearman_correlation.append(coef)\n",
    "            cosine_similarity.append(\n",
    "                np.dot(explanation_DP, explanation_NO_DP)\n",
    "                / (np.linalg.norm(explanation_DP) * np.linalg.norm(explanation_NO_DP))\n",
    "            )\n",
    "            tau, p_value = kendalltau(explanation_DP, explanation_NO_DP)\n",
    "            kendall_tau.append(tau)\n",
    "\n",
    "        feature_agreement = agreement_fraction(\n",
    "            attrA=np.array(loaded_data[\"explanations_NO_DP\"]),\n",
    "            attrB=np.array(loaded_data[f\"explanations_{privacy_level}\"]),\n",
    "            k=5,\n",
    "            metric=\"feature\",\n",
    "        )\n",
    "        rank_agreement = agreement_fraction(\n",
    "            attrA=np.array(loaded_data[\"explanations_NO_DP\"]),\n",
    "            attrB=np.array(loaded_data[f\"explanations_{privacy_level}\"]),\n",
    "            k=5,\n",
    "            metric=\"rank\",\n",
    "        )\n",
    "        sign_agreement = agreement_fraction(\n",
    "            attrA=np.array(loaded_data[\"explanations_NO_DP\"]),\n",
    "            attrB=np.array(loaded_data[f\"explanations_{privacy_level}\"]),\n",
    "            k=5,\n",
    "            metric=\"sign\",\n",
    "        )\n",
    "        signed_rank_agreement = agreement_fraction(\n",
    "            attrA=np.array(loaded_data[\"explanations_NO_DP\"]),\n",
    "            attrB=np.array(loaded_data[f\"explanations_{privacy_level}\"]),\n",
    "            k=5,\n",
    "            metric=\"signedrank\",\n",
    "        )\n",
    "        pairwise_rank_agreement_score = pairwise_rank_agreement(\n",
    "            attrA=np.array(loaded_data[\"explanations_NO_DP\"]),\n",
    "            attrB=np.array(loaded_data[f\"explanations_{privacy_level}\"]),\n",
    "        )\n",
    "        rank_corr = rankcorr(\n",
    "            attrA=np.array(loaded_data[\"explanations_NO_DP\"]),\n",
    "            attrB=np.array(loaded_data[f\"explanations_{privacy_level}\"]),\n",
    "        )\n",
    "\n",
    "        metrics[f\"L2_{privacy_level}\"] = L2_distances\n",
    "        metrics[f\"spearman_{privacy_level}\"] = spearman_correlation\n",
    "        metrics[f\"cosine_{privacy_level}\"] = cosine_similarity\n",
    "        metrics[f\"kendall_{privacy_level}\"] = kendall_tau\n",
    "        metrics[f\"feature_agreement_{privacy_level}\"] = feature_agreement\n",
    "        metrics[f\"rank_agreement_{privacy_level}\"] = rank_agreement\n",
    "        metrics[f\"sign_agreement_{privacy_level}\"] = sign_agreement\n",
    "        metrics[f\"signed_rank_agreement_{privacy_level}\"] = signed_rank_agreement\n",
    "        metrics[f\"pairwise_rank_agreement_{privacy_level}\"] = (\n",
    "            pairwise_rank_agreement_score\n",
    "        )\n",
    "        metrics[f\"rank_corr_{privacy_level}\"] = rank_corr\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_single_explainer(\n",
    "    X_test,\n",
    "    surrogate_NO_DP,\n",
    "    surrogate_DP,\n",
    "    selected_features,\n",
    "    all_features,\n",
    "    top_k=None,\n",
    "):\n",
    "    metrics = {}\n",
    "    L2_distances = []\n",
    "    spearman_correlation = []\n",
    "    cosine_similarity = []\n",
    "    kendall_tau = []\n",
    "    for index in range(len(X_test)):\n",
    "        selected_indexes = [\n",
    "            all_features.index(feature) for feature in selected_features\n",
    "        ]\n",
    "\n",
    "        explanation_NO_DP = surrogate_NO_DP[\"explanations_NO_DP\"][index][\n",
    "            selected_indexes\n",
    "        ]\n",
    "\n",
    "        explanation_DP = surrogate_DP[f\"explanations_NO_DP\"][index][\n",
    "            selected_indexes\n",
    "        ]\n",
    "\n",
    "        L2_distances.append(np.linalg.norm(explanation_NO_DP - explanation_DP))\n",
    "        coef, p = spearmanr(explanation_DP, explanation_NO_DP)\n",
    "        spearman_correlation.append(coef)\n",
    "        cosine_similarity.append(\n",
    "            np.dot(explanation_DP, explanation_NO_DP)\n",
    "            / (np.linalg.norm(explanation_DP) * np.linalg.norm(explanation_NO_DP))\n",
    "        )\n",
    "        tau, p_value = kendalltau(explanation_DP, explanation_NO_DP)\n",
    "        kendall_tau.append(tau)\n",
    "\n",
    "    feature_agreement = agreement_fraction(\n",
    "        attrA=np.array(surrogate_NO_DP[\"explanations_NO_DP\"]),\n",
    "        attrB=np.array(surrogate_DP[f\"explanations_NO_DP\"]),\n",
    "        k=5,\n",
    "        metric=\"feature\",\n",
    "    )\n",
    "    rank_agreement = agreement_fraction(\n",
    "        attrA=np.array(surrogate_NO_DP[\"explanations_NO_DP\"]),\n",
    "        attrB=np.array(surrogate_DP[f\"explanations_NO_DP\"]),\n",
    "        k=5,\n",
    "        metric=\"rank\",\n",
    "    )\n",
    "    sign_agreement = agreement_fraction(\n",
    "        attrA=np.array(surrogate_NO_DP[\"explanations_NO_DP\"]),\n",
    "        attrB=np.array(surrogate_DP[f\"explanations_NO_DP\"]),\n",
    "        k=5,\n",
    "        metric=\"sign\",\n",
    "    )\n",
    "    signed_rank_agreement = agreement_fraction(\n",
    "        attrA=np.array(surrogate_NO_DP[\"explanations_NO_DP\"]),\n",
    "        attrB=np.array(surrogate_DP[f\"explanations_NO_DP\"]),\n",
    "        k=5,\n",
    "        metric=\"signedrank\",\n",
    "    )\n",
    "    pairwise_rank_agreement_score = pairwise_rank_agreement(\n",
    "        attrA=np.array(surrogate_NO_DP[\"explanations_NO_DP\"]),\n",
    "        attrB=np.array(surrogate_DP[f\"explanations_NO_DP\"]),\n",
    "    )\n",
    "    rank_corr = rankcorr(\n",
    "        attrA=np.array(surrogate_NO_DP[\"explanations_NO_DP\"]),\n",
    "        attrB=np.array(surrogate_DP[f\"explanations_NO_DP\"]),\n",
    "    )\n",
    "\n",
    "    metrics[f\"L2\"] = np.mean(L2_distances)\n",
    "    metrics[f\"spearman\"] = np.mean(spearman_correlation)\n",
    "    metrics[f\"cosine\"] = np.mean(cosine_similarity)\n",
    "    metrics[f\"kendall\"] = np.mean(kendall_tau)\n",
    "    metrics[f\"feature_agreement\"] = np.mean(feature_agreement)\n",
    "    metrics[f\"rank_agreement\"] = np.mean(rank_agreement)\n",
    "    metrics[f\"sign_agreement\"] = np.mean(sign_agreement)\n",
    "    metrics[f\"signed_rank_agreement\"] = np.mean(signed_rank_agreement)\n",
    "    metrics[f\"pairwise_rank_agreement\"] = np.mean(\n",
    "        pairwise_rank_agreement_score\n",
    "    )\n",
    "    metrics[f\"rank_corr\"] = np.mean(rank_corr)\n",
    "\n",
    "\n",
    "    metrics[f\"L2_std\"] = np.std(L2_distances)\n",
    "    metrics[f\"spearman_std\"] = np.std(spearman_correlation)\n",
    "    metrics[f\"cosine_std\"] = np.std(cosine_similarity)\n",
    "    metrics[f\"kendall_std\"] = np.std(kendall_tau)\n",
    "    metrics[f\"feature_agreement_std\"] = np.std(feature_agreement)\n",
    "    metrics[f\"rank_agreement_std\"] = np.std(rank_agreement)\n",
    "    metrics[f\"sign_agreement_std\"] = np.std(sign_agreement)\n",
    "    metrics[f\"signed_rank_agreement_std\"] = np.std(signed_rank_agreement)\n",
    "    metrics[f\"pairwise_rank_agreement_std\"] = np.std(\n",
    "        pairwise_rank_agreement_score\n",
    "    )\n",
    "    metrics[f\"rank_corr_std\"] = np.std(rank_corr)\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_explanations(data, explainer_privacy_levels, rescale_to):\n",
    "    # Normalize explanations\n",
    "    num_data = len(data[f\"explanations_NO_DP\"])\n",
    "    min_max = [[float(\"inf\"), -float(\"inf\")] for _ in range(num_data)]\n",
    "\n",
    "    for privacy_level in explainer_privacy_levels:\n",
    "        current_explanation = data[f\"explanations_{privacy_level}\"]\n",
    "        for index, explanation in enumerate(current_explanation):\n",
    "            max_value = np.max(explanation)\n",
    "            min_value = np.min(explanation)\n",
    "            if max_value > min_max[index][1]:\n",
    "                min_max[index][1] = max_value\n",
    "            if min_value < min_max[index][0]:\n",
    "                min_max[index][0] = min_value\n",
    "\n",
    "    # normalize the explanations\n",
    "    for privacy_level in explainer_privacy_levels:\n",
    "        current_explanation = data[f\"explanations_{privacy_level}\"]\n",
    "\n",
    "        current_explanations = []\n",
    "        for index, explanation in enumerate(current_explanation):\n",
    "            maximum_value = min_max[index][1]\n",
    "            minimum_value = min_max[index][0]\n",
    "\n",
    "            if rescale_to == \"min_max\":\n",
    "                current_explanation = (explanation - min(explanation)) / (\n",
    "                    maximum_value - minimum_value\n",
    "                )\n",
    "\n",
    "                current_explanation = (\n",
    "                    ((explanation - min(explanation)) * (maximum_value - minimum_value))\n",
    "                    / (max(explanation) - min(explanation))\n",
    "                ) + minimum_value\n",
    "\n",
    "                # (((OldValue - OldMin) * (NewMax - NewMin)) / (OldMax - OldMin)) + NewMin\n",
    "\n",
    "            elif rescale_to == \"minus_one_one\":\n",
    "                current_explanation = (\n",
    "                    2 * (explanation - minimum_value) / (maximum_value - minimum_value)\n",
    "                    - 1\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid rescale_to value. Use 'min_max' or 'minus_one_one'.\"\n",
    "                )\n",
    "            current_explanations.append(current_explanation)\n",
    "        data[f\"explanations_{privacy_level}\"] = current_explanations\n",
    "\n",
    "    # Normalize explanations\n",
    "\n",
    "    minimum_value = float(\"inf\")\n",
    "    maximum_value = float(\"-inf\")\n",
    "    for privacy_level in explainer_privacy_levels:\n",
    "        current_explanation = data[f\"explanations_{privacy_level}\"]\n",
    "        for explanation in current_explanation:\n",
    "            max_value = np.max(explanation)\n",
    "            min_value = np.min(explanation)\n",
    "            if max_value > maximum_value:\n",
    "                maximum_value = max_value\n",
    "            if min_value < minimum_value:\n",
    "                minimum_value = min_value\n",
    "\n",
    "    print(minimum_value, maximum_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes = [\"25\", \"23\", \"19\", \"11\", \"4\", \"45\", \"26\", \"9\", \"29\", \"16\"]\n",
    "explainer_privacy_levels = [\"NO_DP\", \"DP_01\", \"DP_05\", \"DP_1\", \"DP_2\", \"DP_5\"]\n",
    "bb_privacy_levels = [\"NO_DP\"]\n",
    "feature_names = [\n",
    "    \"age\",\n",
    "    \"household_position\",\n",
    "    \"household_size\",\n",
    "    \"prev_residence_place\",\n",
    "    \"citizenship\",\n",
    "    \"country_birth\",\n",
    "    \"edu_level\",\n",
    "    \"economic_status\",\n",
    "    \"cur_eco_activity\",\n",
    "    \"Marital_status\",\n",
    "    \"sex_binary\",\n",
    "    \"bias\",\n",
    "]\n",
    "\n",
    "base_path = \"../../../artifacts/dutch_NO_DP_surrogate/\"\n",
    "store_path = \"/raid/lcorbucci/folktables/dutch/\"\n",
    "surrogate_name = \"dutch_surrogate_DP_1\"\n",
    "surrogate_NO_DP_name = \"dutch_surrogate_NO_DP\"\n",
    "fed_dir = \"../../../data/dutch/federated/\"\n",
    "dataset_name = \"dutch\"\n",
    "num_features = 12\n",
    "top_k_private_bb = [\n",
    "    \"edu_level\",\n",
    "    \"sex_binary\",\n",
    "    \"age\",\n",
    "    \"economic_status\",\n",
    "]\n",
    "metrics_folder = \"../../../artifacts/dutch_NO_DP_surrogate/metrics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data_surrogate_DP = load_data(\n",
    "    private_model=True,\n",
    "    private_surrogate=True,\n",
    "    explainer_privacy_levels=explainer_privacy_levels,\n",
    "    base_path=base_path,\n",
    "    model_name=\"bb_DP_1\",\n",
    "    surrogate_name=surrogate_name,\n",
    "    explainer_name=\"explainer\",\n",
    "    device=\"cuda\",\n",
    "    num_features=num_features,\n",
    "    no_dp_explainer=\"explainer_NO_DP\",\n",
    ")\n",
    "\n",
    "loaded_data_surrogate_NO_DP = load_data(\n",
    "    private_model=True,\n",
    "    private_surrogate=False,\n",
    "    explainer_privacy_levels=explainer_privacy_levels[1:],\n",
    "    base_path=base_path,\n",
    "    model_name=\"bb_DP_1\",\n",
    "    surrogate_name=surrogate_NO_DP_name,\n",
    "    explainer_name=\"explainer\",\n",
    "    device=\"cuda\",\n",
    "    num_features=num_features,\n",
    "    no_dp_explainer=\"explainer_NO_DP_surrogate_NO_DP\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prepare_data_FL(\n",
    "    test_clients=test_nodes,\n",
    "    batch_size=32,\n",
    "    dataset_name=dataset_name,\n",
    "    num_workers=0,\n",
    "    fed_dir=fed_dir,\n",
    "    device=\"cuda\",\n",
    "    num_features=num_features,\n",
    "    seed=42,\n",
    "    surrogate=loaded_data_surrogate_DP[\"surrogate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with BB model\n",
    "\n",
    "loaded_data_surrogate_DP[\"model_predictions\"] = get_model_predictions(\n",
    "    model=loaded_data_surrogate_DP[\"model\"], X_test=X_test\n",
    ")\n",
    "\n",
    "loaded_data_surrogate_NO_DP[\"model_predictions\"] = copy.copy(loaded_data_surrogate_DP[\"model_predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use this matrix of ones to\n",
    "ones = torch.ones(1, 12, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "surrogate_output = []\n",
    "for x in X_test:\n",
    "    output = loaded_data_surrogate_DP[\"surrogate\"](\n",
    "        torch.tensor(x).to(\"cuda\"), ones[: len(x)].to(\"cuda\")\n",
    "    )\n",
    "    surrogate_output.append(output.cpu().detach().numpy())\n",
    "\n",
    "surrogate_output = np.array(surrogate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_outputs = [np.argmax(item, axis=1)[0] for item in surrogate_output]\n",
    "\n",
    "model_predictions = [item.item() for item in loaded_data_surrogate_DP[\"model_predictions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare surrogate_output with model_predictions and compute the model fidelity\n",
    "\n",
    "model_fidelity = np.sum(\n",
    "    np.array(surrogate_outputs) == np.array(model_predictions)\n",
    ") / len(model_predictions)\n",
    "\n",
    "print(f\"Model fidelity: {model_fidelity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the base value\n",
    "\n",
    "base_value = np.mean(X_test)\n",
    "base_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_explanations(\n",
    "    model_predictions=loaded_data_surrogate_DP[\"model_predictions\"],\n",
    "    X_test=X_test,\n",
    "    loaded_data=loaded_data_surrogate_DP,\n",
    "    base_value=base_value,\n",
    "    base_path=\"/raid/lcorbucci/folktables/dutch/\",\n",
    "    explainer_privacy_levels=explainer_privacy_levels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_explanations(\n",
    "    model_predictions=loaded_data_surrogate_NO_DP[\"model_predictions\"],\n",
    "    X_test=X_test,\n",
    "    loaded_data=loaded_data_surrogate_NO_DP,\n",
    "    base_value=base_value,\n",
    "    base_path=\"/raid/lcorbucci/folktables/dutch/\",\n",
    "    explainer_privacy_levels=explainer_privacy_levels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data_surrogate_DP[\"explanations_NO_DP\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data_surrogate_NO_DP[\"explanations_NO_DP\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_explanations(\n",
    "#     data=loaded_data_NO_DP_bb,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     rescale_to=\"min_max\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_surrogate_DP,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_NO_DP\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_surrogate_NO_DP,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_NO_DP\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_private_bb = [\n",
    "    \"edu_level\",\n",
    "    \"sex_binary\",\n",
    "    \"age\",\n",
    "    \"economic_status\",\n",
    "]\n",
    "\n",
    "# top_k_private_bb_surrogate_DP = [\n",
    "#     \"edu_level\",\n",
    "#     \"sex_binary\",\n",
    "#     \"age\",\n",
    "#     \"household_size\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_DP_bb,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_DP_01\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_DP_bb,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_DP_05\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_DP_bb,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_DP_1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_DP_bb,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_DP_2\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_base_explainer_shap_values(\n",
    "#     loaded_data=loaded_data_DP_bb,\n",
    "#     explainer_privacy_levels=explainer_privacy_levels,\n",
    "#     X_test=X_test,\n",
    "#     feature_names=feature_names,\n",
    "#     title=\"Mean SHAP Values of Features for NON Private Model\",\n",
    "#     explainer_name=\"explanations_DP_5\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(f\"{metrics_folder}/metrics_surrogate_DP.pkl\"):\n",
    "    metrics_surrogate_DP = dill.load(\n",
    "        open(\n",
    "            f\"{metrics_folder}/metrics_surrogate_DP.pkl\",\n",
    "            \"rb\",\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    metrics_surrogate_DP = compute_metrics(\n",
    "        X_test,\n",
    "        loaded_data_surrogate_DP,\n",
    "        explainer_privacy_levels,\n",
    "        top_k_private_bb,\n",
    "        feature_names,\n",
    "    )\n",
    "    dill.dump(\n",
    "        metrics_surrogate_DP,\n",
    "        open(\n",
    "            f\"{metrics_folder}/metrics_surrogate_DP.pkl\",\n",
    "            \"wb\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "if os.path.isfile(f\"{metrics_folder}/metrics_surrogate_NO_DP.pkl\"):\n",
    "    metrics_surrogate_NO_DP = dill.load(\n",
    "        open(\n",
    "            f\"{metrics_folder}/metrics_surrogate_NO_DP.pkl\",\n",
    "            \"rb\",\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    metrics_surrogate_NO_DP = compute_metrics(\n",
    "        X_test,\n",
    "        loaded_data_surrogate_NO_DP,\n",
    "        explainer_privacy_levels,\n",
    "        top_k_private_bb,\n",
    "        feature_names,\n",
    "    )\n",
    "    dill.dump(\n",
    "        metrics_surrogate_NO_DP,\n",
    "        open(\n",
    "            f\"{metrics_folder}/metrics_surrogate_NO_DP.pkl\",\n",
    "            \"wb\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "if os.path.isfile(f\"{metrics_folder}/metrics_single_explainer_comparison.pkl\"):\n",
    "    metrics_single_explainer = dill.load(\n",
    "        open(\n",
    "            f\"{metrics_folder}/metrics_single_explainer_comparison.pkl\",\n",
    "            \"rb\",\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    metrics_single_explainer = compute_metrics_single_explainer(\n",
    "        X_test,\n",
    "        loaded_data_surrogate_NO_DP,\n",
    "        loaded_data_surrogate_DP,\n",
    "        top_k_private_bb,\n",
    "        feature_names,\n",
    "    )\n",
    "    dill.dump(\n",
    "        metrics_single_explainer,\n",
    "        open(\n",
    "            f\"{metrics_folder}/metrics_single_explainer_comparison.pkl\",\n",
    "            \"wb\",\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_single_explainer = compute_metrics_single_explainer(\n",
    "#         X_test,\n",
    "#         loaded_data_surrogate_NO_DP,\n",
    "#         loaded_data_surrogate_DP,\n",
    "#         top_k_private_bb,\n",
    "#         feature_names,\n",
    "#     )\n",
    "# dill.dump(metrics_single_explainer, open(\"../../../artifacts/dutch_federated_cross_device_DP_everywhere_test/metrics/metrics_single_explainer_comparison.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean, result_std, result_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"L2_\"\n",
    ")\n",
    "\n",
    "cosine_mean, cosine_std, cosine_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"cosine_\"\n",
    ")\n",
    "\n",
    "spearman_mean, spearman_std, spearman_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"spearman_\"\n",
    ")\n",
    "\n",
    "kendaltau_mean, kendaltau_std, kendaltau_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"kendall_\"\n",
    ")\n",
    "\n",
    "feature_agreement_mean, feature_agreement_std, feature_agreement_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:],\n",
    "    metrics_surrogate_DP,\n",
    "    \"feature_agreement_\",\n",
    ")\n",
    "\n",
    "rank_agreement_mean, rank_agreement_std, rank_agreement_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"rank_agreement_\"\n",
    ")\n",
    "\n",
    "sign_agreement_mean, sign_agreement_std, sign_agreement_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"sign_agreement_\"\n",
    ")\n",
    "\n",
    "signed_rank_agreement_mean, signed_rank_agreement_std, signed_rank_agreement_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:],\n",
    "    metrics_surrogate_DP,\n",
    "    \"signed_rank_agreement_\",\n",
    ")\n",
    "\n",
    "pairwise_rank_agreement_t_mean, pairwise_rank_agreement_t_std, pairwise_rank_agreement_t_std_error = (\n",
    "    create_dictionaries_metrics(\n",
    "        explainer_privacy_levels[1:],\n",
    "        metrics_surrogate_DP,\n",
    "        \"pairwise_rank_agreement_\",\n",
    "    )\n",
    ")\n",
    "\n",
    "rank_corr_mean, rank_corr_std, rank_corr_std_error = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_DP, \"rank_corr_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean_surrogate_NO_DP, result_std_surrogate_NO_DP, result_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"L2_\"\n",
    ")\n",
    "\n",
    "cosine_mean_surrogate_NO_DP, cosine_std_surrogate_NO_DP, cosine_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"cosine_\"\n",
    ")\n",
    "\n",
    "spearman_mean_surrogate_NO_DP, spearman_std_surrogate_NO_DP, spearman_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"spearman_\"\n",
    ")\n",
    "\n",
    "kendaltau_mean_surrogate_NO_DP, kendaltau_std_surrogate_NO_DP, kendaltau_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"kendall_\"\n",
    ")\n",
    "\n",
    "feature_agreement_mean_surrogate_NO_DP, feature_agreement_std_surrogate_NO_DP, feature_agreement_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:],\n",
    "    metrics_surrogate_NO_DP,\n",
    "    \"feature_agreement_\",\n",
    ")\n",
    "\n",
    "rank_agreement_mean_surrogate_NO_DP, rank_agreement_std_surrogate_NO_DP, rank_agreement_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"rank_agreement_\"\n",
    ")\n",
    "\n",
    "sign_agreement_mean_surrogate_NO_DP, sign_agreement_std_surrogate_NO_DP, sign_agreement_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"sign_agreement_\"\n",
    ")\n",
    "\n",
    "signed_rank_agreement_mean_surrogate_NO_DP, signed_rank_agreement_std_surrogate_NO_DP, signed_rank_agreement_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:],\n",
    "    metrics_surrogate_NO_DP,\n",
    "    \"signed_rank_agreement_\",\n",
    ")\n",
    "\n",
    "pairwise_rank_agreement_t_mean_surrogate_NO_DP, pairwise_rank_agreement_t_std_surrogate_NO_DP, pairwise_rank_agreement_t_std_error_surrogate_NO_DP = (\n",
    "    create_dictionaries_metrics(\n",
    "        explainer_privacy_levels[1:],\n",
    "        metrics_surrogate_NO_DP,\n",
    "        \"pairwise_rank_agreement_\",\n",
    "    )\n",
    ")\n",
    "\n",
    "rank_corr_mean_surrogate_NO_DP, rank_corr_std_surrogate_NO_DP, rank_corr_std_error_surrogate_NO_DP = create_dictionaries_metrics(\n",
    "    explainer_privacy_levels[1:], metrics_surrogate_NO_DP, \"rank_corr_\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(results_mean.keys()),\n",
    "    results_mean=results_mean,\n",
    "    results_std=result_std,\n",
    "    results_mean_surrogate_NO_DP=results_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=result_std_surrogate_NO_DP,\n",
    "    y_label=rf\"Mean $\\ell_2$ distance\",\n",
    "    title=rf\"Mean $\\ell_2$ Distance vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0, 0.5),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_l2_distance.png\",\n",
    "    jittering=True,\n",
    "    horizontal_line= metrics_single_explainer[\"L2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(results_mean.keys()),\n",
    "    results_mean=results_mean,\n",
    "    results_std=result_std,\n",
    "    results_mean_surrogate_NO_DP=results_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=result_std_surrogate_NO_DP,\n",
    "    y_label=rf\"Mean $\\ell_2$ distance\",\n",
    "    title=rf\"Mean $\\ell_2$ Distance vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0, 0.5),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_l2_distance_filled.png\",\n",
    "    jittering=True,\n",
    "    horizontal_line= metrics_single_explainer[\"L2\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"L2_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(cosine_mean.keys()),\n",
    "    results_mean=cosine_mean,\n",
    "    results_std=cosine_std,\n",
    "    results_mean_surrogate_NO_DP=cosine_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=cosine_std_surrogate_NO_DP,\n",
    "    y_label=\"Cosine Similarity\",\n",
    "    title=rf\"Cosine Similarity vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.2, 1.1),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_cosine.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"cosine\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(cosine_mean.keys()),\n",
    "    results_mean=cosine_mean,\n",
    "    results_std=cosine_std,\n",
    "    results_mean_surrogate_NO_DP=cosine_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=cosine_std_surrogate_NO_DP,\n",
    "    y_label=\"Cosine Similarity\",\n",
    "    title=rf\"Cosine Similarity vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.2, 1.1),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_cosine_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"cosine\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"cosine_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(spearman_mean.keys()),\n",
    "    results_mean=spearman_mean,\n",
    "    results_std=spearman_std,\n",
    "    results_mean_surrogate_NO_DP=spearman_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=spearman_std_surrogate_NO_DP,\n",
    "    y_label=\"Spearman Correlation\",\n",
    "    title=\"Spearman correlation vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.7, 1.2),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_spearman.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"spearman\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(spearman_mean.keys()),\n",
    "    results_mean=spearman_mean,\n",
    "    results_std=spearman_std,\n",
    "    results_mean_surrogate_NO_DP=spearman_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=spearman_std_surrogate_NO_DP,\n",
    "    y_label=\"Spearman Correlation\",\n",
    "    title=\"Spearman correlation vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.7, 1.2),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_spearman_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"spearman\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"spearman_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kendall correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(kendaltau_mean.keys()),\n",
    "    results_mean=kendaltau_mean,\n",
    "    results_std=kendaltau_std,\n",
    "    results_mean_surrogate_NO_DP=kendaltau_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=kendaltau_std_surrogate_NO_DP,\n",
    "    y_label=\"Kendaltau\",\n",
    "    title=\"Kendaltau vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.6, 1.2),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_kendal.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"kendall\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(kendaltau_mean.keys()),\n",
    "    results_mean=kendaltau_mean,\n",
    "    results_std=kendaltau_std,\n",
    "    results_mean_surrogate_NO_DP=kendaltau_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=kendaltau_std_surrogate_NO_DP,\n",
    "    y_label=\"Kendaltau\",\n",
    "    title=\"Kendaltau vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.6, 1.2),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_kendal_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"kendall\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"kendall_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(feature_agreement_mean.keys()),\n",
    "    results_mean=feature_agreement_mean,\n",
    "    results_std=feature_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=feature_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=feature_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Feature Agreement\",\n",
    "    title=\"Feature Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0.25, 1.05),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_feat_agreement.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"feature_agreement\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(feature_agreement_mean.keys()),\n",
    "    results_mean=feature_agreement_mean,\n",
    "    results_std=feature_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=feature_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=feature_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Feature Agreement\",\n",
    "    title=\"Feature Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0.25, 1.05),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_feat_agreement_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"feature_agreement\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"feature_agreement_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(rank_agreement_mean.keys()),\n",
    "    results_mean=rank_agreement_mean,\n",
    "    results_std=rank_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=rank_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=rank_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Rank Agreement\",\n",
    "    title=\"Rank Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.1, 0.9),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_rank_agreement.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"rank_agreement\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(rank_agreement_mean.keys()),\n",
    "    results_mean=rank_agreement_mean,\n",
    "    results_std=rank_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=rank_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=rank_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Rank Agreement\",\n",
    "    title=\"Rank Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.1, 0.9),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_rank_agreement_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"rank_agreement\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"rank_agreement_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(sign_agreement_mean.keys()),\n",
    "    results_mean=sign_agreement_mean,\n",
    "    results_std=sign_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=sign_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=sign_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Sign Agreement\",\n",
    "    title=\"Sign Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0.05, 1.05),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_sign_agreement.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"sign_agreement\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(sign_agreement_mean.keys()),\n",
    "    results_mean=sign_agreement_mean,\n",
    "    results_std=sign_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=sign_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=sign_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Sign Agreement\",\n",
    "    title=\"Sign Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0.05, 1.05),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_sign_agreement_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"sign_agreement\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"sign_agreement_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign Rank Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(signed_rank_agreement_mean.keys()),\n",
    "    results_mean=signed_rank_agreement_mean,\n",
    "    results_std=signed_rank_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=signed_rank_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=signed_rank_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Sign Rank Agreement\",\n",
    "    title=\"Sign Rank Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.1, 0.9),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_sign_rank_agreement.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"signed_rank_agreement\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(signed_rank_agreement_mean.keys()),\n",
    "    results_mean=signed_rank_agreement_mean,\n",
    "    results_std=signed_rank_agreement_std,\n",
    "    results_mean_surrogate_NO_DP=signed_rank_agreement_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=signed_rank_agreement_std_surrogate_NO_DP,\n",
    "    y_label=\"Sign Rank Agreement\",\n",
    "    title=\"Sign Rank Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.1, 0.9),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_sign_rank_agreement_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"signed_rank_agreement\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"signed_rank_agreement_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(pairwise_rank_agreement_t_mean.keys()),\n",
    "    results_mean=pairwise_rank_agreement_t_mean,\n",
    "    results_std=pairwise_rank_agreement_t_std,\n",
    "    results_mean_surrogate_NO_DP=pairwise_rank_agreement_t_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=pairwise_rank_agreement_t_std_surrogate_NO_DP,\n",
    "    y_label=\"Pairwise Rank Agreement\",\n",
    "    title=\"Pairwise Rank Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0.38, 0.95),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_pairwise_rank_agreement.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"pairwise_rank_agreement\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(pairwise_rank_agreement_t_mean.keys()),\n",
    "    results_mean=pairwise_rank_agreement_t_mean,\n",
    "    results_std=pairwise_rank_agreement_t_std,\n",
    "    results_mean_surrogate_NO_DP=pairwise_rank_agreement_t_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=pairwise_rank_agreement_t_std_surrogate_NO_DP,\n",
    "    y_label=\"Pairwise Rank Agreement\",\n",
    "    title=\"Pairwise Rank Agreement vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(0.38, 0.95),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_pairwise_rank_agreement_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"pairwise_rank_agreement\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"pairwise_rank_agreement_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(rank_corr_mean.keys()),\n",
    "    results_mean=rank_corr_mean,\n",
    "    results_std=rank_corr_std,\n",
    "    results_mean_surrogate_NO_DP=rank_corr_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=rank_corr_std_surrogate_NO_DP,\n",
    "    y_label=\"Rank Correlation\",\n",
    "    title=\"Rank Correlation vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.3, 1),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_rank_correlation.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"rank_corr\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(rank_corr_mean.keys()),\n",
    "    results_mean=rank_corr_mean,\n",
    "    results_std=rank_corr_std,\n",
    "    results_mean_surrogate_NO_DP=rank_corr_mean_surrogate_NO_DP,\n",
    "    results_std_surrogate_NO_DP=rank_corr_std_surrogate_NO_DP,\n",
    "    y_label=\"Rank Correlation\",\n",
    "    title=\"Rank Correlation vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.3, 1),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_rank_correlation_filled.png\",\n",
    "    horizontal_line= metrics_single_explainer[\"rank_corr\"],\n",
    "    horizontal_line_std=metrics_single_explainer[\"rank_corr_std\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_mean = {}\n",
    "faithfulness_std = {}\n",
    "faithfulness_mean_no_DP = {}\n",
    "faithfulness_std_no_DP = {}\n",
    "\n",
    "faithfulness_single_value_mean = np.mean(abs(np.array(loaded_data_surrogate_NO_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_DP[\"faithfulness_NO_DP\"])))\n",
    "faithfulness_single_value_std = np.std(abs(np.array(loaded_data_surrogate_NO_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_DP[\"faithfulness_NO_DP\"])))\n",
    "\n",
    "faithfulness_single_values_NO_DP, faithfulness_single_values = np.array(loaded_data_surrogate_NO_DP[\"faithfulness_NO_DP\"]), np.array(loaded_data_surrogate_DP[\"faithfulness_NO_DP\"])\n",
    "\n",
    "faithfulness= {}\n",
    "faithfulness_no_DP = {}\n",
    "\n",
    "for epsilon in explainer_privacy_levels[1:]:\n",
    "    value = str(epsilon).split(\"_\")[1]\n",
    "    faithfulness_mean[f\"Epsilon {value}\"] = np.mean(abs(np.array(loaded_data_surrogate_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_DP[f\"faithfulness_{epsilon}\"])))\n",
    "    faithfulness_std[f\"Epsilon {value}\"] = np.std(abs(np.array(loaded_data_surrogate_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_DP[f\"faithfulness_{epsilon}\"])))\n",
    "\n",
    "    faithfulness_mean_no_DP[f\"Epsilon {value}\"] = np.mean(abs(np.array(loaded_data_surrogate_NO_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_NO_DP[f\"faithfulness_{epsilon}\"])))\n",
    "    faithfulness_std_no_DP[f\"Epsilon {value}\"] = np.std(abs(np.array(loaded_data_surrogate_NO_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_NO_DP[f\"faithfulness_{epsilon}\"])))\n",
    "\n",
    "    faithfulness[f\"Epsilon {value}\"] = np.array(loaded_data_surrogate_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_DP[f\"faithfulness_{epsilon}\"])\n",
    "\n",
    "    faithfulness_no_DP[f\"Epsilon {value}\"] = np.array(loaded_data_surrogate_NO_DP[\"faithfulness_NO_DP\"]) - np.array(loaded_data_surrogate_NO_DP[f\"faithfulness_{epsilon}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "value = \"01\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution of faithfulness_single_value using seaborn and KDE\n",
    "sns.kdeplot(faithfulness_single_values,  cumulative=True)\n",
    "sns.kdeplot(faithfulness_single_values_NO_DP,  cumulative=True)\n",
    "\n",
    "for epsilon in explainer_privacy_levels[1:]:\n",
    "    value = str(epsilon).split(\"_\")[1]\n",
    "    sns.kdeplot(faithfulness[f\"Epsilon {value}\"], cumulative=True)\n",
    "\n",
    "plt.xlabel('Faithfulness Single Value', fontsize=25)\n",
    "plt.ylabel('Density', fontsize=25)\n",
    "plt.title('Distribution of Faithfulness Single Value', fontsize=25)\n",
    "plt.legend([\"DP\", \"NO_DP\"] + [f\"Epsilon {value}\" for value in [\"01\", \"05\", \"1\", \"2\", \"5\"]], fontsize=10)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "value = \"01\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the distribution of faithfulness_single_value using seaborn and KDE\n",
    "sns.kdeplot(faithfulness_single_values, shade=True)\n",
    "sns.kdeplot(faithfulness_single_values_NO_DP, shade=True)\n",
    "\n",
    "for epsilon in explainer_privacy_levels[1:]:\n",
    "    value = str(epsilon).split(\"_\")[1]\n",
    "    sns.kdeplot(faithfulness[f\"Epsilon {value}\"], shade=True)\n",
    "\n",
    "plt.xlabel('Faithfulness Single Value', fontsize=25)\n",
    "plt.ylabel('Density', fontsize=25)\n",
    "plt.title('Distribution of Faithfulness Single Value', fontsize=25)\n",
    "plt.legend([\"DP\", \"NO_DP\"] + [f\"Epsilon {value}\" for value in [\"01\", \"05\", \"1\", \"2\", \"5\"]], fontsize=10)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(rank_corr_mean.keys()),\n",
    "    results_mean=faithfulness_mean,\n",
    "    results_std=faithfulness_std,\n",
    "    results_mean_surrogate_NO_DP=faithfulness_mean_no_DP,\n",
    "    results_std_surrogate_NO_DP=faithfulness_std_no_DP,\n",
    "    y_label=\"Rank Correlation\",\n",
    "    title=f\"$\\Delta$ Fairness vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.05, 0.75),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_faithfulness.png\",\n",
    "    horizontal_line= faithfulness_single_value_mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_bar(  # Extracting the means and standard errors\n",
    "    labels=list(rank_corr_mean.keys()),\n",
    "    results_mean=faithfulness_mean,\n",
    "    results_std=faithfulness_std,\n",
    "    results_mean_surrogate_NO_DP=faithfulness_mean_no_DP,\n",
    "    results_std_surrogate_NO_DP=faithfulness_std_no_DP,\n",
    "    y_label=\"Rank Correlation\",\n",
    "    title=f\"$\\Delta$ Fairness vs ($\\epsilon$, $\\delta$)-DP\",\n",
    "    y_lim=(-0.05, 0.75),\n",
    "    x_ticks=[\n",
    "        r\"$\\epsilon=0.1$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=0.5$\",  # , \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=1$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=2$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "        r\"$\\epsilon=5$\",  # \\delta = 4 \\times 10^{-4}$)\",\n",
    "    ],\n",
    "    file_name=\"./dutch_images/dutch_faithfulness_filled.png\",\n",
    "    horizontal_line= faithfulness_single_value_mean,\n",
    "    horizontal_line_std=faithfulness_single_value_std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the epsilon values\n",
    "epsilon_values = [\"01\", \"05\", \"1\", \"2\", \"5\"]\n",
    "epsilon_values_list = [\"0.1\", \"0.5\", \"1\", \"2\", \"5\"]\n",
    "\n",
    "metrics_dict = {\n",
    "    \"L2 Distance\": [f\"{results_mean[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{result_std[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values] + [f\"{results_mean_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{result_std_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values],\n",
    "    \"Cosine Similarity\": [f\"{cosine_mean[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{cosine_std[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values] + [f\"{cosine_mean_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{cosine_std_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values],\n",
    "    \"Feature Agreement\": [f\"{feature_agreement_mean[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{feature_agreement_std[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values] + [f\"{feature_agreement_mean_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{feature_agreement_std_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values],\n",
    "    \"Sign Agreement\": [f\"{sign_agreement_mean[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{sign_agreement_std[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values] + [f\"{sign_agreement_mean_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{sign_agreement_std_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values],\n",
    "    \"Rank Correlation\": [f\"{rank_corr_mean[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{rank_corr_std[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values] + [f\"{rank_corr_mean_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{rank_corr_std_surrogate_NO_DP[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values],\n",
    "    \"Faithfulness\": [f\"{faithfulness_mean[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{faithfulness_std[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values] + [f\"{faithfulness_mean_no_DP[f'Epsilon {eps}']:.2f}\"+r\"$\\pm$\"+f\"{faithfulness_std_no_DP[f'Epsilon {eps}']:.2f}\" for eps in epsilon_values],\n",
    "}\n",
    "\n",
    "# Define the index for the DataFrame\n",
    "index = [rf\"($\\epsilon={eps}, \\delta = 4 \\times 10^{{-4}}$)\" for eps in epsilon_values_list] + [rf\"($\\epsilon={eps}, \\delta = 4 \\times 10^{{-4}}$) (NO_DP)\" for eps in epsilon_values_list]\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=index)\n",
    "\n",
    "print(metrics_df.to_latex())\n",
    "\n",
    "epsilon_values_list = [\"0.1\", \"0.5\", \"1\", \"2\", \"5\", \"0.1 NO DP\", \"0.5 NO DP\", \"1 NO DP\", \"2 NO DP\", \"5 NO DP\"]\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=[rf\"(\\epsilon={eps}, \\delta = 4 \\times 10^{-4}$)\" for eps in epsilon_values_list])\n",
    "\n",
    "print(metrics_df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
