{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "engaged-interference",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  # https://github.com/slundberg/shap\n",
    "import shapreg  # https://github.com/iancovert/shapley-regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    *shap.datasets.adult(), test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Data scaling\n",
    "num_features = X_train.shape[1]\n",
    "feature_names = X_train.columns.tolist()\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train.values)\n",
    "X_val = ss.transform(X_val.values)\n",
    "X_test = ss.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-citizen",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b59782",
   "metadata": {},
   "source": [
    "A gradient boosting model is trained on the adult dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"census_model.pkl\"):\n",
    "    print(\"Loading saved model\")\n",
    "    with open(\"census_model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # Setup\n",
    "    params = {\n",
    "        \"max_bin\": 512,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"num_leaves\": 10,\n",
    "        \"verbose\": -1,\n",
    "        \"min_data\": 100,\n",
    "        \"boost_from_average\": True,\n",
    "    }\n",
    "\n",
    "    # More setup\n",
    "    d_train = lgb.Dataset(X_train, label=Y_train)\n",
    "    d_val = lgb.Dataset(X_val, label=Y_val)\n",
    "\n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        d_train,\n",
    "        10000,\n",
    "        valid_sets=[d_val],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(1000)],\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    with open(\"census_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97024d57",
   "metadata": {},
   "source": [
    "# Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b779ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, BatchSampler\n",
    "from fastshap.utils import UniformSampler, DatasetRepeat\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def validate(surrogate, loss_fn, data_loader):\n",
    "    \"\"\"\n",
    "    Calculate mean validation loss.\n",
    "\n",
    "    Args:\n",
    "      loss_fn: loss function.\n",
    "      data_loader: data loader.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        device = next(surrogate.surrogate.parameters()).device\n",
    "        mean_loss = 0\n",
    "        N = 0\n",
    "\n",
    "        for x, y, S in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            S = S.to(device)\n",
    "            pred = surrogate(x, S)\n",
    "            loss = loss_fn(pred, y)\n",
    "            N += len(x)\n",
    "            mean_loss += len(x) * (loss - mean_loss) / N\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def generate_labels(dataset, model, batch_size):\n",
    "    \"\"\"\n",
    "    Generate prediction labels for a set of inputs.\n",
    "\n",
    "    Args:\n",
    "      dataset: dataset object.\n",
    "      model: predictive model.\n",
    "      batch_size: minibatch size.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Setup.\n",
    "        preds = []\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "            device = next(model.parameters()).device\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        for (x,) in loader:\n",
    "            pred = model(x.to(device)).cpu()\n",
    "            preds.append(pred)\n",
    "\n",
    "    return torch.cat(preds)\n",
    "\n",
    "\n",
    "class Surrogate:\n",
    "    \"\"\"\n",
    "    Wrapper around surrogate model.\n",
    "\n",
    "    Args:\n",
    "      surrogate: surrogate model.\n",
    "      num_features: number of features.\n",
    "      groups: (optional) feature groups, represented by a list of lists.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, surrogate, num_features, groups=None):\n",
    "        # Store surrogate model.\n",
    "        self.surrogate = surrogate\n",
    "\n",
    "        # Store feature groups.\n",
    "        if groups is None:\n",
    "            self.num_players = num_features\n",
    "            self.groups_matrix = None\n",
    "        else:\n",
    "            # Verify groups.\n",
    "            inds_list = []\n",
    "            for group in groups:\n",
    "                inds_list += list(group)\n",
    "            assert np.all(np.sort(inds_list) == np.arange(num_features))\n",
    "\n",
    "            # Map groups to features.\n",
    "            self.num_players = len(groups)\n",
    "            device = next(surrogate.parameters()).device\n",
    "            self.groups_matrix = torch.zeros(\n",
    "                len(groups), num_features, dtype=torch.float32, device=device\n",
    "            )\n",
    "            for i, group in enumerate(groups):\n",
    "                self.groups_matrix[i, group] = 1\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        batch_size,\n",
    "        max_epochs,\n",
    "        loss_fn,\n",
    "        validation_samples=1,\n",
    "        validation_batch_size=None,\n",
    "        lr=1e-3,\n",
    "        min_lr=1e-5,\n",
    "        lr_factor=0.5,\n",
    "        lookback=5,\n",
    "        training_seed=None,\n",
    "        validation_seed=None,\n",
    "        bar=False,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train surrogate model.\n",
    "\n",
    "        Args:\n",
    "          train_data: training data with inputs and the original model's\n",
    "            predictions (np.ndarray tuple, torch.Tensor tuple,\n",
    "            torch.utils.data.Dataset).\n",
    "          val_data: validation data with inputs and the original model's\n",
    "            predictions (np.ndarray tuple, torch.Tensor tuple,\n",
    "            torch.utils.data.Dataset).\n",
    "          batch_size: minibatch size.\n",
    "          max_epochs: maximum training epochs.\n",
    "          loss_fn: loss function (e.g., fastshap.KLDivLoss).\n",
    "          validation_samples: number of samples per validation example.\n",
    "          validation_batch_size: validation minibatch size.\n",
    "          lr: initial learning rate.\n",
    "          min_lr: minimum learning rate.\n",
    "          lr_factor: learning rate decrease factor.\n",
    "          lookback: lookback window for early stopping.\n",
    "          training_seed: random seed for training.\n",
    "          validation_seed: random seed for generating validation data.\n",
    "          verbose: verbosity.\n",
    "        \"\"\"\n",
    "        # Set up train dataset.\n",
    "        if isinstance(train_data, tuple):\n",
    "            x_train, y_train = train_data\n",
    "            if isinstance(x_train, np.ndarray):\n",
    "                x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "                y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "            train_set = TensorDataset(x_train, y_train)\n",
    "        elif isinstance(train_data, Dataset):\n",
    "            train_set = train_data\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"train_data must be either tuple of tensors or a \" \"PyTorch Dataset\"\n",
    "            )\n",
    "\n",
    "        # Set up train data loader.\n",
    "        random_sampler = RandomSampler(\n",
    "            train_set,\n",
    "            replacement=True,\n",
    "            num_samples=int(np.ceil(len(train_set) / batch_size)) * batch_size,\n",
    "        )\n",
    "        batch_sampler = BatchSampler(\n",
    "            random_sampler, batch_size=batch_size, drop_last=True\n",
    "        )\n",
    "        train_loader = DataLoader(train_set, batch_sampler=batch_sampler)\n",
    "\n",
    "        # Set up validation dataset.\n",
    "        sampler = UniformSampler(self.num_players)\n",
    "        if validation_seed is not None:\n",
    "            torch.manual_seed(validation_seed)\n",
    "        S_val = sampler.sample(len(val_data) * validation_samples)\n",
    "\n",
    "        if isinstance(val_data, tuple):\n",
    "            x_val, y_val = val_data\n",
    "            if isinstance(x_val, np.ndarray):\n",
    "                x_val = torch.tensor(x_val, dtype=torch.float32)\n",
    "                y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "            x_val_repeat = x_val.repeat(validation_samples, 1)\n",
    "            y_val_repeat = y_val.repeat(validation_samples, 1)\n",
    "            val_set = TensorDataset(x_val_repeat, y_val_repeat, S_val)\n",
    "        elif isinstance(val_data, Dataset):\n",
    "            val_set = DatasetRepeat([val_data, TensorDataset(S_val)])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"val_data must be either tuple of tensors or a \" \"PyTorch Dataset\"\n",
    "            )\n",
    "\n",
    "        if validation_batch_size is None:\n",
    "            validation_batch_size = batch_size\n",
    "        val_loader = DataLoader(val_set, batch_size=validation_batch_size)\n",
    "\n",
    "        # Setup for training.\n",
    "        surrogate = self.surrogate\n",
    "        device = next(surrogate.parameters()).device\n",
    "        optimizer = optim.Adam(surrogate.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            factor=lr_factor,\n",
    "            patience=lookback // 2,\n",
    "            min_lr=min_lr,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        best_loss = validate(self, loss_fn, val_loader).item()\n",
    "        best_epoch = 0\n",
    "        best_model = deepcopy(surrogate)\n",
    "        loss_list = [best_loss]\n",
    "        if training_seed is not None:\n",
    "            torch.manual_seed(training_seed)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            # Batch iterable.\n",
    "            if bar:\n",
    "                batch_iter = tqdm(train_loader, desc=\"Training epoch\")\n",
    "            else:\n",
    "                batch_iter = train_loader\n",
    "\n",
    "            for x, y in batch_iter:\n",
    "                # Prepare data.\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Generate subsets.\n",
    "                S = sampler.sample(batch_size).to(device=device)\n",
    "\n",
    "                # Make predictions.\n",
    "                pred = self.__call__(x, S)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                # Optimizer step.\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                surrogate.zero_grad()\n",
    "\n",
    "            # Evaluate validation loss.\n",
    "            self.surrogate.eval()\n",
    "            val_loss = validate(self, loss_fn, val_loader).item()\n",
    "            self.surrogate.train()\n",
    "\n",
    "            # Print progress.\n",
    "            if verbose:\n",
    "                print(\"----- Epoch = {} -----\".format(epoch + 1))\n",
    "                print(\"Val loss = {:.4f}\".format(val_loss))\n",
    "                print(\"\")\n",
    "            scheduler.step(val_loss)\n",
    "            loss_list.append(val_loss)\n",
    "\n",
    "            # Check if best model.\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = deepcopy(surrogate)\n",
    "                best_epoch = epoch\n",
    "                if verbose:\n",
    "                    print(\"New best epoch, loss = {:.4f}\".format(val_loss))\n",
    "                    print(\"\")\n",
    "            elif epoch - best_epoch == lookback:\n",
    "                if verbose:\n",
    "                    print(\"Stopping early\")\n",
    "                break\n",
    "\n",
    "        # Clean up.\n",
    "        for param, best_param in zip(surrogate.parameters(), best_model.parameters()):\n",
    "            param.data = best_param.data\n",
    "        self.loss_list = loss_list\n",
    "        self.surrogate.eval()\n",
    "\n",
    "    def train_original_model(\n",
    "        self,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        original_model,\n",
    "        batch_size,\n",
    "        max_epochs,\n",
    "        loss_fn,\n",
    "        validation_samples=1,\n",
    "        validation_batch_size=None,\n",
    "        lr=1e-3,\n",
    "        min_lr=1e-5,\n",
    "        lr_factor=0.5,\n",
    "        lookback=5,\n",
    "        training_seed=None,\n",
    "        validation_seed=None,\n",
    "        bar=False,\n",
    "        verbose=False,\n",
    "        optimizer=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train surrogate model with labels provided by the original model.\n",
    "\n",
    "        Args:\n",
    "          train_data: training data with inputs only (np.ndarray, torch.Tensor,\n",
    "            torch.utils.data.Dataset).\n",
    "          val_data: validation data with inputs only (np.ndarray, torch.Tensor,\n",
    "            torch.utils.data.Dataset).\n",
    "          original_model: original predictive model (e.g., torch.nn.Module).\n",
    "          batch_size: minibatch size.\n",
    "          max_epochs: maximum training epochs.\n",
    "          loss_fn: loss function (e.g., fastshap.KLDivLoss).\n",
    "          validation_samples: number of samples per validation example.\n",
    "          validation_batch_size: validation minibatch size.\n",
    "          lr: initial learning rate.\n",
    "          min_lr: minimum learning rate.\n",
    "          lr_factor: learning rate decrease factor.\n",
    "          lookback: lookback window for early stopping.\n",
    "          training_seed: random seed for training.\n",
    "          validation_seed: random seed for generating validation data.\n",
    "          verbose: verbosity.\n",
    "        \"\"\"\n",
    "        if not optimizer:\n",
    "            raise ValueError(\"optimizer must be provided\")\n",
    "\n",
    "        # Set up validation dataset.\n",
    "        sampler = UniformSampler(self.num_players)\n",
    "        if validation_seed is not None:\n",
    "            torch.manual_seed(validation_seed)\n",
    "        S_val = sampler.sample(len(val_data) * validation_samples)\n",
    "        if validation_batch_size is None:\n",
    "            validation_batch_size = batch_size\n",
    "\n",
    "        if isinstance(val_data, np.ndarray):\n",
    "            val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "\n",
    "        if isinstance(val_data, torch.Tensor):\n",
    "            # Generate validation labels.\n",
    "            y_val = generate_labels(\n",
    "                TensorDataset(val_data), original_model, validation_batch_size\n",
    "            )\n",
    "            y_val_repeat = y_val.repeat(\n",
    "                validation_samples, *[1 for _ in y_val.shape[1:]]\n",
    "            )\n",
    "\n",
    "            # Create dataset.\n",
    "            val_data_repeat = val_data.repeat(validation_samples, 1)\n",
    "            val_set = TensorDataset(val_data_repeat, y_val_repeat, S_val)\n",
    "        elif isinstance(val_data, Dataset):\n",
    "            # Generate validation labels.\n",
    "            y_val = generate_labels(val_data, original_model, validation_batch_size)\n",
    "            y_val_repeat = y_val.repeat(\n",
    "                validation_samples, *[1 for _ in y_val.shape[1:]]\n",
    "            )\n",
    "\n",
    "            # Create dataset.\n",
    "            val_set = DatasetRepeat([val_data, TensorDataset(y_val_repeat, S_val)])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"val_data must be either tuple of tensors or a \" \"PyTorch Dataset\"\n",
    "            )\n",
    "\n",
    "        val_loader = DataLoader(val_set, batch_size=validation_batch_size)\n",
    "\n",
    "        # Setup for training.\n",
    "        surrogate = self.surrogate\n",
    "        device = next(surrogate.parameters()).device\n",
    "\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            factor=lr_factor,\n",
    "            patience=lookback // 2,\n",
    "            min_lr=min_lr,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        best_loss = validate(self, loss_fn, val_loader).item()\n",
    "        best_epoch = 0\n",
    "        best_model = deepcopy(surrogate)\n",
    "        loss_list = [best_loss]\n",
    "        if training_seed is not None:\n",
    "            torch.manual_seed(training_seed)\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            print(\"Epoch\", epoch)\n",
    "            # Batch iterable.\n",
    "            if bar:\n",
    "                batch_iter = tqdm(train_loader, desc=\"Training epoch\")\n",
    "            else:\n",
    "                batch_iter = train_loader\n",
    "\n",
    "            for (x,) in batch_iter:\n",
    "                # Prepare data.\n",
    "                x = x.to(device)\n",
    "\n",
    "                # Get original model prediction.\n",
    "                with torch.no_grad():\n",
    "                    y = original_model(x)\n",
    "\n",
    "                # Generate subsets.\n",
    "                S = sampler.sample(batch_size).to(device=device)\n",
    "\n",
    "                # Make predictions.\n",
    "                pred = self.__call__(x, S)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                # Optimizer step.\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                surrogate.zero_grad()\n",
    "\n",
    "            # Evaluate validation loss.\n",
    "            self.surrogate.eval()\n",
    "            val_loss = validate(self, loss_fn, val_loader).item()\n",
    "            self.surrogate.train()\n",
    "\n",
    "            # Print progress.\n",
    "            if verbose:\n",
    "                print(\"----- Epoch = {} -----\".format(epoch + 1))\n",
    "                print(\"Val loss = {:.4f}\".format(val_loss))\n",
    "                print(\"\")\n",
    "            scheduler.step(val_loss)\n",
    "            loss_list.append(val_loss)\n",
    "\n",
    "            # Check if best model.\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = deepcopy(surrogate)\n",
    "                best_epoch = epoch\n",
    "                if verbose:\n",
    "                    print(\"New best epoch, loss = {:.4f}\".format(val_loss))\n",
    "                    print(\"\")\n",
    "            elif epoch - best_epoch == lookback:\n",
    "                if verbose:\n",
    "                    print(\"Stopping early\")\n",
    "                break\n",
    "\n",
    "        # Clean up.\n",
    "        for param, best_param in zip(surrogate.parameters(), best_model.parameters()):\n",
    "            param.data = best_param.data\n",
    "        self.loss_list = loss_list\n",
    "        self.surrogate.eval()\n",
    "\n",
    "    def __call__(self, x, S):\n",
    "        \"\"\"\n",
    "        Evaluate surrogate model.\n",
    "\n",
    "        Args:\n",
    "          x: input examples.\n",
    "          S: coalitions.\n",
    "        \"\"\"\n",
    "        if self.groups_matrix is not None:\n",
    "            S = torch.mm(S, self.groups_matrix)\n",
    "\n",
    "        return self.surrogate((x, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b08ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(train_data, batch_size):\n",
    "    # Set up train dataset.\n",
    "    if isinstance(train_data, np.ndarray):\n",
    "        train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(train_data, torch.Tensor):\n",
    "        train_set = TensorDataset(train_data)\n",
    "    elif isinstance(train_data, Dataset):\n",
    "        train_set = train_data\n",
    "    else:\n",
    "        raise ValueError(\"train_data must be either tensor or a \" \"PyTorch Dataset\")\n",
    "\n",
    "    # Set up train data loader.\n",
    "    random_sampler = RandomSampler(\n",
    "        train_set,\n",
    "        replacement=True,\n",
    "        num_samples=int(np.ceil(len(train_set) / batch_size)) * batch_size,\n",
    "    )\n",
    "    batch_sampler = BatchSampler(random_sampler, batch_size=batch_size, drop_last=True)\n",
    "    train_loader = DataLoader(train_set, batch_sampler=batch_sampler)\n",
    "    return train_loader, random_sampler, batch_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-setup",
   "metadata": {},
   "source": [
    "# Train surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastshap.utils import MaskLayer1d\n",
    "from fastshap import KLDivLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus.validators import ModuleValidator\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model\n",
    "if os.path.isfile(\"census_surrogate.pt\"):\n",
    "    # Set up original model\n",
    "    def original_model(x):\n",
    "        pred = model.predict(x.cpu().numpy())\n",
    "        pred = np.stack([1 - pred, pred]).T\n",
    "        return torch.tensor(pred, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    print(\"Loading saved surrogate model\")\n",
    "    surr = torch.load(\"census_surrogate.pt\").to(device)\n",
    "    surrogate = Surrogate(surr, num_features)\n",
    "    surrogate.train_original_model(\n",
    "        X_train,  # We pass the training dataset of the black box to the surrogate object\n",
    "        X_val,  # We pass the validation dataset of the black box to the surrogate object\n",
    "        original_model,  # black box we want to explain\n",
    "        batch_size=64,\n",
    "        max_epochs=100,\n",
    "        loss_fn=KLDivLoss(),\n",
    "        validation_samples=10,  # number of samples per validation example\n",
    "        validation_batch_size=10000,  # size of the mini batch\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    batch_size = 64\n",
    "    train_loader, random_sampler, batch_sampler = setup_data(\n",
    "        train_data=X_train, batch_size=batch_size\n",
    "    )\n",
    "    print(\"Setup data\")\n",
    "\n",
    "    # Create surrogate model\n",
    "    surr = nn.Sequential(\n",
    "        MaskLayer1d(value=0, append=True),\n",
    "        nn.Linear(2 * num_features, 128),\n",
    "        # nn.ELU(inplace=True),\n",
    "        # nn.Linear(128, 128),\n",
    "        # nn.ELU(inplace=True),\n",
    "        nn.Linear(128, 2),\n",
    "    )\n",
    "    surr = ModuleValidator.fix(surr)\n",
    "    ModuleValidator.validate(surr, strict=False)\n",
    "    print(\"Fixing model\")\n",
    "\n",
    "    MAX_GRAD_NORM = 1.2\n",
    "    EPSILON = 50.0\n",
    "    DELTA = 1e-5\n",
    "    lr = 1e-3\n",
    "    EPOCHS = 100\n",
    "\n",
    "    privacy_engine = PrivacyEngine()\n",
    "\n",
    "    optimizer = optim.Adam(surr.parameters(), lr=lr)\n",
    "    print(\"Created optimizer\")\n",
    "\n",
    "    surr, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "        module=surr,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        epochs=EPOCHS,\n",
    "        target_epsilon=EPSILON,\n",
    "        target_delta=DELTA,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "    )\n",
    "\n",
    "    print(\"Made model private\")\n",
    "\n",
    "    surr = surr.to(device)\n",
    "\n",
    "    # Set up surrogate object: we pass the model that we have defined as\n",
    "    # surrogate and the number of input features of the training dataset\n",
    "    # we used to train the black box.\n",
    "    surrogate = Surrogate(surr, num_features)\n",
    "    print(\"Created surrogate\")\n",
    "\n",
    "    # Set up original model\n",
    "    def original_model(x):\n",
    "        pred = model.predict(x.cpu().numpy())\n",
    "        pred = np.stack([1 - pred, pred]).T\n",
    "        return torch.tensor(pred, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # Train\n",
    "    # What happens inside the train_original_model\n",
    "    # - A UniformSampler is created to sample the data from the validation set\n",
    "    # - Given the validation set, this is multiplied by validation_samples and\n",
    "    # the UniformSampler will then create a matrix of size (len(validation_set) * validation_samples, num_features)\n",
    "    # Inside this matrix there will be 1 or 0, the value is based on a random threshold. This is to mask some of the features\n",
    "    # - For each sample in this \"augmented\" matrix we need to compute the corresponding\n",
    "    # prediction of the black box model. This is done by calling the original_model using the\n",
    "    # validation set and then augmenting the prediction to match the size of the augmented groups_matrix\n",
    "    # - Then a validation_set is created with the repeated samples of the original validation set\n",
    "    # the corresponding repeated predictions and the masked features matrix (S_val)\n",
    "    # It is important to notice that the validation set will have the following shape:\n",
    "    # [repeated_val_data, repeated_predictions, masked_features]\n",
    "    # - Then we set the optimizer. Note that this is another hyperparameter but it is set as Adam\n",
    "    # directly from the code.\n",
    "    # - The training loop starts:\n",
    "    #    - We iterate over the batches of the training data\n",
    "    #    - We compute the prediction of the original model for each of the batches\n",
    "    #    - We compute the prediction of the surrogate model on the batch masked using a sampling mask (S)\n",
    "    #    - We compute the loss using the prediction of the surrogate model and the prediction of the original model\n",
    "    #    - We compute the gradients and update the surrogate model\n",
    "    #    - After each batch, we evaluate the surrogate model on the validation set\n",
    "\n",
    "    print(\"Starting training original model\")\n",
    "    surrogate.train_original_model(\n",
    "        X_train,  # We pass the training dataset of the black box to the surrogate object\n",
    "        X_val,  # We pass the validation dataset of the black box to the surrogate object\n",
    "        original_model,  # black box we want to explain\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=EPOCHS,\n",
    "        loss_fn=KLDivLoss(),\n",
    "        validation_samples=10,  # this number is multiplied with the length of the validation dataset\n",
    "        validation_batch_size=10000,  # size of the mini batch\n",
    "        verbose=True,\n",
    "        lr=lr,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        random_sampler=random_sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        bar=True,\n",
    "    )\n",
    "\n",
    "    # Save surrogate\n",
    "    # surr.cpu()\n",
    "    # torch.save(surr, 'census_surrogate.pt')\n",
    "    # surr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ee9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modern-nightlife",
   "metadata": {},
   "source": [
    "# Train FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastshap import FastSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model\n",
    "if os.path.isfile(\"census_explainer.pt\"):\n",
    "    print(\"Loading saved explainer model\")\n",
    "    explainer = torch.load(\"census_explainer.pt\").to(device)\n",
    "    fastshap = FastSHAP(\n",
    "        explainer, surrogate, normalization=\"additive\", link=nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Create explainer model\n",
    "    explainer = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(128, 2 * num_features),\n",
    "    ).to(device)\n",
    "\n",
    "    # Set up FastSHAP object\n",
    "    fastshap = FastSHAP(\n",
    "        explainer, surrogate, normalization=\"additive\", link=nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    fastshap.train(\n",
    "        X_train,\n",
    "        X_val[:100],\n",
    "        batch_size=32,\n",
    "        num_samples=32,\n",
    "        max_epochs=200,\n",
    "        validation_samples=128,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Save explainer\n",
    "    explainer.cpu()\n",
    "    torch.save(explainer, \"census_explainer.pt\")\n",
    "    explainer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-sacramento",
   "metadata": {},
   "source": [
    "# Compare with KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for KernelSHAP\n",
    "def imputer(x, S):\n",
    "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "    S = torch.tensor(S, dtype=torch.float32, device=device)\n",
    "    pred = surrogate(x, S).softmax(dim=-1)\n",
    "    return pred.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example\n",
    "ind = np.random.choice(len(X_test))\n",
    "x = X_test[ind : ind + 1]\n",
    "y = int(Y_test[ind])\n",
    "\n",
    "# Run FastSHAP\n",
    "fastshap_values = fastshap.shap_values(x)[0]\n",
    "\n",
    "# Run KernelSHAP to convergence\n",
    "game = shapreg.games.PredictionGame(imputer, x)\n",
    "shap_values, all_results = shapreg.shapley.ShapleyRegression(\n",
    "    game,\n",
    "    batch_size=32,\n",
    "    paired_sampling=False,\n",
    "    detect_convergence=True,\n",
    "    bar=True,\n",
    "    return_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(figsize=(9, 5.5))\n",
    "\n",
    "# Bar chart\n",
    "width = 0.75\n",
    "kernelshap_iters = 128\n",
    "plt.bar(\n",
    "    np.arange(num_features) - width / 3,\n",
    "    shap_values.values[:, y],\n",
    "    width / 3,\n",
    "    label=\"True SHAP values\",\n",
    "    color=\"tab:gray\",\n",
    ")\n",
    "plt.bar(\n",
    "    np.arange(num_features),\n",
    "    fastshap_values[:, y],\n",
    "    width / 3,\n",
    "    label=\"FastSHAP\",\n",
    "    color=\"tab:green\",\n",
    ")\n",
    "plt.bar(\n",
    "    np.arange(num_features) + width / 3,\n",
    "    all_results[\"values\"][list(all_results[\"iters\"]).index(kernelshap_iters)][:, y],\n",
    "    width / 3,\n",
    "    label=\"KernelSHAP @ {}\".format(kernelshap_iters),\n",
    "    color=\"tab:red\",\n",
    ")\n",
    "\n",
    "# Annotations\n",
    "plt.legend(fontsize=16)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.ylabel(\"SHAP Values\", fontsize=16)\n",
    "plt.title(\"Census Explanation Example\", fontsize=18)\n",
    "plt.xticks(\n",
    "    np.arange(num_features),\n",
    "    feature_names,\n",
    "    rotation=35,\n",
    "    rotation_mode=\"anchor\",\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-earthquake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
